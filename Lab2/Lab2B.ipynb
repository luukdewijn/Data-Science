{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab2B: The Country Risk case: Principal Components Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the application of Principal Components Analysis.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1. }}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1. }}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kmeans algorithm from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# np.set_printoptions(precision=3)  # print results with 3 decimals behind the decimal point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data, scaling, and K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "raw = pd.read_csv('./Country_Risk_2019_Data.csv')\n",
    "X = raw[['Corruption', 'Peace', 'Legal', 'GDP Growth']]\n",
    "X = (X - X.mean()) / X.std()\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation matrix:\")\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means with k=3\n",
    "For comparison with the results later, we repeat the K-means clustering on Peace, Legal, and GDP Growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=1, n_init=10)\n",
    "kmeans.fit(X.iloc[:,1:])  # skipping column Corruption\n",
    "km_labels = kmeans.labels_\n",
    "print(\"cluster labels: \\n\", km_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Components Analysis by Sklearn\n",
    "### First: how you might find what you need in the Sklearn documentation\n",
    "**N.B.** If you have difficulty finding your way, you should read this carefully and follow the steps.\n",
    "\n",
    "You might start by having a peak at [PCA in the Skearn User Guide](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca). The User Guide has lots of useful (also background) information, though this is not always very accessible for novices. In this case, we read after a few lines that \"`PCA` is implemented as a *transformer* object that learns components in its `fit` method.\" After the PCA plot of the Iris data the discussion quickly leaves our scope. Time to move on. \n",
    "\n",
    "By clicking on [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) \n",
    "we get to the  [API page for `sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) (Apparently, PCA is part of the `sklearn.decomposition` module on Matrix Decomposition; good to know).\n",
    "\n",
    "Of the **parameters** of (objects in the class) `PCA` you probably only need (to know) `n_components`, but you should scroll down to the **attributes**, of which you will recognize some from the PCA-terminology. Scroll even further down to **Methods** and scan them. Probably you only have use for the methods **fit**, **fit_transform**, and **transform**. Look over their descriptions and you might (like I) be a bit confused about whether the methods return an object or work on the object passed as arguments. \n",
    "\n",
    "You need to invest in reading these pages to get more familiar. The key is not to be overwhelmed and note what rings a bell; there are many many options and details that are outside our scope and can therefore be igonored. Sklearn is very structured and organized, which means that if you invest in understanding that structure and organization, you will learn to find your way. One way to do that is to go to the [`Getting Started` pages](https://scikit-learn.org/stable/getting_started.html). Very highly recommended!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1}$ Let's first reproduce the Excel results reported in **Table 2.11** and **Table 2.12**. In the last table also show a row with the explained variances. An easy way to produce a nice table is to use a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2.11\n",
    "pca_full = PCA().fit(X)\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2.12 including explained variance\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2}$ Also make a plot of the cumulative explained variance against the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering based on PC1 versus K-means with k=3\n",
    "\n",
    "$\\ex{3}$ Determine what percentage of the variance is explained by the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is quite a lot we are going to do K-means clustering on the factor scores of the first principal component.\n",
    "\n",
    "$\\ex{4}$ Generate the factor scores for PC1 and use K-means clustering to obtain 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_labels = None  # Use this name for the cluster labels that you find\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the clusterings\n",
    "We are going to make a few comparisons and a contingency table. The (adjusted) Rand index is a measure of how well the clusterings agree (possibly after permuting the labeling); for identical clusters both indices are 1.0. The contingency table shows the frequencies of the label combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rindex = metrics.rand_score(km_labels, PC_labels)\n",
    "print(f\"The Rand index is: {rindex:4.3}\")\n",
    "arindex = metrics.adjusted_rand_score(km_labels, PC_labels)\n",
    "print(f\"The adjusted Rand index is: {arindex:4.3}\")\n",
    "\n",
    "cm = metrics.cluster.contingency_matrix(km_labels, PC_labels)\n",
    "con_tab = pd.DataFrame(cm, columns= [0,1,2], index=[0,1,2])\n",
    "con_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for the PCA we should interchange labels 1 and 2. The results below show that this does not change the Rand indices (by their label permutation invariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interchange labels 1 <-> 2 and repeat (to check that nothing really changes):\n",
    "rel = np.array([0, 2, 1]) \n",
    "PC_labels = rel[PC_labels]\n",
    "rindex = metrics.rand_score(km_labels, PC_labels)\n",
    "print(f\"The Rand index is: {rindex:4.3}\")\n",
    "arindex = metrics.adjusted_rand_score(km_labels, PC_labels)\n",
    "print(f\"The adjusted Rand index is: {arindex:4.3}\")\n",
    "\n",
    "cm = metrics.cluster.contingency_matrix(km_labels, PC_labels)\n",
    "con_tab = pd.DataFrame(cm, columns= [0,1,2], index=[0,1,2])\n",
    "con_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5}$ Repeat this comparison, but now using 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{6}$ Relabel again, if necessary, to get the contingency table \"right.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack([km_labels, PC_labels]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{1}$ Comment on the results. It seems that the PCA does not reproduce the K-means results (even if you use more components). Is this surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1be3e\">\n",
    "\n",
    "Write your answer here:\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix (**optional**). Principal Components Analysis using Numpy.linalg.eig\n",
    "\n",
    "**Note:** In class the linear algebra behind PCA was briefly mentioned and the rest of this section shows how this can be implemented with tools from Numpy.linalg. You are not required to know this, or even read this. On the other hand, if you prefer to use this: that's fine, as long as you get the right answers.\n",
    "\n",
    "### Computing the principal components\n",
    "First compute eigenvalues and eigenvectors for the correlation matrix. They (might) need to be reordered, the order should be from the largest to the lowest eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(X.corr())\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr = eigenvalues.argsort() # this function returns an index list that would order the eigenvalues\n",
    "decr = incr[::-1]    # reverse the order: we need largest FIRST\n",
    "eigenvalues_ord = eigenvalues[decr]\n",
    "eigenvectors_ord = eigenvectors[:,decr]  # reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check:\n",
    "print(\"Eigenvalues in decreasing order:\", eigenvalues_ord)\n",
    "print(\"Eigenvectors in the same order:\\n\", eigenvectors_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\"]\n",
    "#convert ordered eigenvectors into dataframe \n",
    "df_eigenvec = pd.DataFrame(data=eigenvectors_ord, columns=columns, \n",
    "                            index=[\"Corruption Index\",\"Peace Index\", \"Legal risk Index\", \"GDP Growth rate\"])\n",
    "# SD of factor scores\n",
    "SD_factor_scores = np.sqrt(eigenvalues_ord)\n",
    "# % of variance \n",
    "variance = 100 * eigenvalues_ord / (np.sum(eigenvalues_ord)) \n",
    "\n",
    "# Convert factor scores and variance into dataframe\n",
    "scores_variance = pd.DataFrame(data = [SD_factor_scores, eigenvalues_ord, variance] , columns = columns, \n",
    "                               index = [\"SD of factor scores\", \"variance of scores\", \"% of total variance\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(df_eigenvec,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(scores_variance,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
