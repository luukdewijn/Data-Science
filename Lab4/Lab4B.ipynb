{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4B: Logistic Regression**\n",
    "\n",
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of logistic regression.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something.\n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}.}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('lendingclub_traindata.xlsx')\n",
    "validation=pd.read_excel('lendingclub_valdata.xlsx')\n",
    "test = pd.read_excel('lendingclub_testdata.xlsx')\n",
    "\n",
    "# 1 = good, 0 = default\n",
    "\n",
    "#give column names\n",
    "cols = ['home_ownership', 'income', 'dti', 'fico', 'loan_status']\n",
    "\n",
    "train.columns = validation.columns = test.columns = cols\n",
    "\n",
    "print(train.head())\n",
    "print(\"--------------------------------\")\n",
    "print (validation.head())\n",
    "print(\"--------------------------------\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been split into training set, validation set, and test set. There are 7000 instances of the training set, 3000 instances of the validation set and 2290 instances of the test set. The four features have been labeled as: home ownership, income, dti and fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target column to create feature only dataset\n",
    "X_train = train.drop('loan_status', 1)\n",
    "X_val = validation.drop('loan_status', 1)\n",
    "X_test = test.drop('loan_status', 1)\n",
    "\n",
    "# Scale data using the mean and standard deviation of the training set. \n",
    "# This is not necessary for the simple logistic regression we will do here \n",
    "# but should be done if L1 or L2 regrularization is carried out\n",
    "Xmean = X_train.mean()\n",
    "Xstd = X_train.std()\n",
    "X_train = (X_train - Xmean) / Xstd\n",
    "X_val = (X_val - Xmean) / Xstd\n",
    "X_test = (X_test - Xmean) / Xstd\n",
    "\n",
    "# store target column as y-variables \n",
    "y_train = train['loan_status']\n",
    "y_val = validation['loan_status']\n",
    "y_test = test['loan_status']\n",
    "\n",
    "#print first five instances for each data set\n",
    "\n",
    "print(X_train.head())\n",
    "print(\"--------------------------------\")\n",
    "print(X_val.head())\n",
    "print(\"--------------------------------\")\n",
    "print(X_test.head())\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape,y_val.shape, X_test.shape, y_test.shape)\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = y_train.value_counts()           # count frequency of different classes in training set\n",
    "freq/sum(freq)*100                      # get percentage of above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Logistic Regression\n",
    "We use Sklearn's `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of logisticregression named LogReg \n",
    "\n",
    "LogReg =  LogisticRegression(penalty=\"none\", solver=\"newton-cg\")     \n",
    "\n",
    "# Fit logististic regression to training set\n",
    "LogReg.fit(X_train, y_train)                                     # fit training data on logistic regression \n",
    "print(LogReg.intercept_, LogReg.coef_)                           # get the coefficients of each features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used on scaled data the model has a bias of 1.416 and coefficients of 0.145, 0.034, -0.324 and 0.363. We now test the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function used is the negative of the average log-likelihood. Below, it is computed for the validation and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_avg_loglik(y, probs):\n",
    "    # Preconditions:\n",
    "    # y: ndarray of 0/1\n",
    "    # probs: two-column ndarray, rows contain [P(Y=0),P(Y=1)]\n",
    "    return np.average(-np.log(y * probs[:,1] + (1-y) * probs[:,0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_train, prob_val, and prob_test are the predicted probabilities for the training set\n",
    "# validation set and test set using the fitted logistic regression model\n",
    "\n",
    "prob_train = LogReg.predict_proba(X_train)\n",
    "prob_val = LogReg.predict_proba(X_val)\n",
    "prob_test = LogReg.predict_proba(X_test)\n",
    "\n",
    "# Calculate the negative of the average loglikelihood for training, validation, and test set\n",
    "\n",
    "cost_func_train = neg_avg_loglik(y_train, prob_train)\n",
    "cost_func_val = neg_avg_loglik(y_val, prob_val)\n",
    "cost_func_test = neg_avg_loglik(y_test, prob_test)\n",
    "\n",
    "print(f'cost function training set   = {cost_func_train:10.8}')\n",
    "print(f'cost function validation set = {cost_func_val:10.8}')\n",
    "print(f'cost function test set       = {cost_func_test:10.8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences don't tell us very much. It would be more meaningful to compare the last two numbers to the result we would get if we computed the cost function when the validation resp the test data set are used to fit the model; those are going to be lower than the numbers above (can you see why?).\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\ex{1}$ a) Fit the logistic regression model to the validation data and then compute `neg_avg_loglik(y_val, prob_val_new)`, where `prob_val_new` are the predicted probabilities from the (new) fitted model. Now compute how much this differs from the results above, express this as: 2 times the difference in log-likelihood. b) Then repeat this for the test data set.\n",
    "\n",
    "The outcomes can be used to perform a formal test about the quality of the (training data) fit. The details are beyond our scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analyst must decide on a criterion for predicting whether loan will be good or default. This involves specifying a threshold By default this threshold is set to 0.5, i.e., loans are separated into good and bad categories according to whether the probability of no default is greater or less than 0.5. However this does not work well for an imbalanced data set such as this. It would predict that all loans are good! We will look at the results for few other thresholds. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{2}$ Complete the code below to get the predicted labels for each probability threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = [0.7, .75, .80, .85]\n",
    "# Create dataframe to store results\n",
    "results = pd.DataFrame(columns=[\"THRESHOLD\", \"accuracy\", \"true pos rate\", \"true neg rate\", \"false pos rate\", \"precision\", \"f-score\"]) # df to store results\n",
    "\n",
    "# Create threshold row\n",
    "results['THRESHOLD'] = THRESHOLD                                                                         \n",
    "             \n",
    "j = 0                                                                                                      \n",
    "\n",
    "# Iterate over the 3 thresholds\n",
    "\n",
    "for i in THRESHOLD:                                                                                       \n",
    "    \n",
    "    # If prob for test set > threshold predict 1\n",
    "    # START ANSWER\n",
    "    # END ANSWER                                 \n",
    "    \n",
    "    \n",
    "    # create confusion matrix \n",
    "    cm = (confusion_matrix(y_test, preds,labels=[1, 0], sample_weight=None) / len(y_test))*100    # confusion matrix (in percentage)\n",
    "    \n",
    "    print('Confusion matrix for threshold =',i)\n",
    "    print(cm)\n",
    "    print(' ')      \n",
    "    \n",
    "    TP = cm[0][0]                                                                                          # True Positives\n",
    "    FN = cm[0][1]                                                                                          # False Positives\n",
    "    FP = cm[1][0]                                                                                          # True Negatives\n",
    "    TN = cm[1][1]                                                                                          # False Negatives\n",
    "        \n",
    "    results.iloc[j,1] = accuracy_score(y_test, preds) \n",
    "    results.iloc[j,2] = recall_score(y_test, preds)\n",
    "    results.iloc[j,3] = TN/(FP+TN)                                                                         # True negative rate\n",
    "    results.iloc[j,4] = FP/(FP+TN)                                                                         # False positive rate\n",
    "    results.iloc[j,5] = precision_score(y_test, preds)\n",
    "    results.iloc[j,6] = f1_score(y_test, preds)\n",
    "   \n",
    "   \n",
    "    j += 1\n",
    "\n",
    "print('ALL METRICS')\n",
    "results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows that there is a trade off betwee the true positive rate and the false positive rate.  \n",
    "We can improve the percentage of good loans we identify only by increasing the percentage of bad that are misclassified.\n",
    "The receiver operating curve (ROC) captures this trade off by considering different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{3}$ Plot the ROC curve for the model and also plot the baseline that can be obtained by random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the receiver operating curve and the AUC measure\n",
    "\n",
    "lr_prob=LogReg.predict_proba(X_test)\n",
    "lr_prob=lr_prob[:, 1]\n",
    "baseline_prob=[0 for _ in range(len(y_test))]\n",
    "baseline_auc=roc_auc_score(y_test, baseline_prob)\n",
    "lr_auc=roc_auc_score(y_test,lr_prob)\n",
    "print(\"AUC random predictions =\", baseline_auc)\n",
    "print(\"AUC predictions from logistic regression model =\", lr_auc)\n",
    "baseline_fpr,baseline_tpr,_=roc_curve(y_test,baseline_prob)\n",
    "lr_fpr,lr_tpr,_=roc_curve(y_test,lr_prob)\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Can we tell how well the model generalizes?\n",
    "There is a difficulty: how could we detect overfitting? The \"difference of log-likelihoods test\" mentioned above was an idea outside our scope. We could try this:\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{4}$ Produce and plot another ROC curve that would give you an idea about how well the fitted model generalizes. Add it to the plot above or make a new one combining the two below. Use a different color for the 2nd ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{1}$ Explain your choice of ROC curve. State any conclusion you think you can draw from the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
