{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f16ab8e0-335e-4305-af28-a0e348643b69",
   "metadata": {},
   "source": [
    "# **Lab 4A: Linear regression and regularization**\n",
    "\n",
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with regularization methods for linear regression.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something.\n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1.}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1.}}$\n",
    "\n",
    "**Goal of the Lab:**    \n",
    "In this lab, we aim to fit a ridge regression model and lasso model to the `Hitters` data, as well as an ElasticNet model. We wish to predict a baseball player's `Salary` based on the statistics associated with performance in the previous year.  From several models we wish to select the best one and determine its RMSE.\n",
    "\n",
    "For more information about the data set, including a description of the features/variables, click [here](https://www.rdocumentation.org/packages/ISLR/versions/1.2/topics/Hitters).\n",
    "\n",
    "**Remark:**  \n",
    "This lab is based on a previous version of what now is Section 6.5 Lab 1 and 2: Linear Models and Regularization methods of _An Introduction to Statitistical Learning_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01113d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# To produce static images embedded in the notebook\n",
    "%matplotlib inline \n",
    "# To produce interactive images embedded in the notebook\n",
    "# %matplotlib notebook\n",
    "\n",
    "# sci-kit learn specifics\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# make Pandas display 2 digits after decimal point; some output then fits in window\n",
    "# change this if you like\n",
    "pd.set_option('display.precision', 2)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac1b2871-dbe6-4204-8d24-4825c0c39c93",
   "metadata": {},
   "source": [
    "## 1. Loading and viewing the `Hitters` data set\n",
    "\n",
    "We import the `Hitters` data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835da837",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = pd.read_csv(\"./Hitters.csv\", index_col = 0)\n",
    "\n",
    "# Display information about the data set\n",
    "hitters.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8228d3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{1}$ Look at the first rows of the dataset, print some summary statistics, and find out which features contains `NAs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96548e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first rows of the data set\n",
    "hitters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return summary statistics for each column\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NA values\n",
    "\n",
    "# START ANSWER \n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07052e3b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{2}$ Which features of the dataset are categorical and what are their (unique) values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables:\n",
    "\n",
    "# START ANSWER \n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206fa72",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the data\n",
    "In this part, we preprocess the `Hitters` data so that it is ready for the data fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718c70f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{3}$ Remove all `NA` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_clean = None\n",
    "\n",
    "# START ANSWER \n",
    "# END ANSWER\n",
    "\n",
    "# Display information about the data set\n",
    "hitters_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96c4c3",
   "metadata": {},
   "source": [
    "We obtain the \"cleaned\" data with 263 rows and 20 columns. This is in agreement with the result in Section 6.5 of the book and also with our earlier observation when calling `hitters.info()`. \n",
    "\n",
    "We continue with transforming the *categorical* variables `League`, `Division`, and `NewLeague` to *indicator* variables. Let's see what methods `Pandas` has to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3265b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummies variable\n",
    "dummies = pd.get_dummies(hitters_clean[[\"League\", \"Division\", \"NewLeague\"]])\n",
    "\n",
    "# First 10 rows of data set\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f37831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return summary statistics for each column\n",
    "dummies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26878bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\ex{4}$ Note that for each categorical variable we should only use *one* of the generated dummies. Replace the categorical columns of `hitters_ind` with the corresponding binary values, as described in the comments below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of hitters_clean data set\n",
    "hitters_ind = hitters_clean.copy()\n",
    "\n",
    "# Replace the columns with their 0/1 values such that\n",
    "# League = 'N' is assigned the value 1 and 'A' is assigned the value 0\n",
    "# Division = 'W' is assigned the value 1 and 'E' is assigned the value 0\n",
    "# NewLeague = 'N' is assigned the value 1 and 'A' is assigned the value 0 \n",
    "\n",
    "# START ANSWER \n",
    "# END ANSWER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdbf62-d1d3-4201-8910-bbc6f6039e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that hitters_clean is not changed, but hitters_ind is:\n",
    "hitters_clean.head()\n",
    "\n",
    "hitters_ind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ab635-60d1-4d1b-bfe5-45f63c6a6c84",
   "metadata": {},
   "source": [
    "### 2A. Splitting and scaling the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd14d0-db6f-4562-ba67-35e54d858165",
   "metadata": {},
   "source": [
    "Below we split the data 60/20/20 into training, validation, and test data.\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{5}$ After the splitting, scale the predictor variables. All three data sets must be scaled with mean and standard deviation of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The design matrix X containing the predictors\n",
    "X = hitters_ind.drop(\"Salary\", axis = 1)\n",
    "colnames = list(X.columns)\n",
    "# The y variable containing the response\n",
    "y = hitters_ind.Salary\n",
    "\n",
    "# Splitting (don't change this)\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size = 0.6, random_state = 1267)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, train_size = 0.5, random_state = 9001)\n",
    "\n",
    "# Standardize the design matrices (don't changes their names)\n",
    "# START ANSWER\n",
    "# END ANSWER \n",
    "\n",
    "X = X_val\n",
    "# Check if X has mean zero and variance 1 for each column\n",
    "print(\"\\nMeans of the standardized X:\")\n",
    "print(X.mean(axis = 0))\n",
    "print(\"\\nVariances of the standardized X:\")\n",
    "print(X.var(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0acb71-420c-4831-a8cc-25a1f972f259",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\q{1}$ The resulting means of the validation and training set are not all close to zero. The variances not all close to 1. However, we _must_ do the scaling this way. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc8c04-2cef-40c6-818a-c18cae4f2c16",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130c45a",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74f211",
   "metadata": {},
   "source": [
    "We perform ridge regression in order to predict the baseball players' salaries based on their performance statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of values for lambda, the tuning parameter\n",
    "lambdas = 10**np.linspace(-2, 10, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35ce49",
   "metadata": {},
   "source": [
    "This range for `lambdas` covers the full range of scenarios from the least squares fit ($\\lambda = 10^{-2} $) to the null model containing only the intercept ($\\lambda = 10^{10}$).\n",
    "For each particular value in `lambdas`, we store a vector of ridge regression coefficients plus an intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeRegr = Ridge(fit_intercept = True)\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefsRR = pd.DataFrame(columns = colnames)\n",
    "coefsRR[\"Intercept\"] = \"\"\n",
    "# Loop through lambdas\n",
    "\n",
    "for index,l in enumerate(lambdas):\n",
    "    # set the ridge model with corresponding lambda value \n",
    "    RidgeRegr.set_params(alpha = l)\n",
    "    # fit the model \n",
    "    RidgeRegr.fit(X_train, y_train)\n",
    "    # Add the coefficients and intercept to the dataframe \n",
    "    coeff_intercept = np.append(RidgeRegr.coef_, RidgeRegr.intercept_)\n",
    "    coefsRR.loc[index, :] = coeff_intercept\n",
    "\n",
    "coefsRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b68180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "plt.figure(figsize = (6, 6))\n",
    "ax = plt.axes()\n",
    "for i in range(len(coefsRR.columns)):\n",
    "    ax.plot(lambdas, coefsRR.iloc[:, i], label=coefsRR.columns[i])\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('estimated coefficients')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.75, 1), ncol=1, fontsize='small')  # Adjust bbox_to_anchor for positioning\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21518efd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\q{2}$ Some coefficients go to zero monotonically, others do not, first growing in size or changing sign before going to zero. How can this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74132cd9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1cd2-7d1e-4d0f-b9f1-3188bb887ab9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{3}$ Why is the intercept not going to zero as $\\lambda$ gets large?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "040c9730-993c-4b1e-9300-6523b110667d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf449718",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{6}$ (Skip this in your first pass; see section 6 for explanation) You might add code in the loop to compute training and validation MSEs and could consider printing them with the L2 norms or plotting them simultaneously versus the lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for specific lambdas;\n",
    "lambdas_sel = lambdas # [705, 11000, 10**7] \n",
    "l2_lambdas_sel = []\n",
    "\n",
    "coefsRR_sel = pd.DataFrame(columns = colnames)\n",
    "coefsRR_sel[\"Intercept\"] = \"\"\n",
    "\n",
    "# Loop through lambdas\n",
    "for index,l in enumerate(lambdas_sel):\n",
    "    RidgeRegr.set_params(alpha = l)\n",
    "    RidgeRegr.fit(X_train, y_train)\n",
    "    coeff_intercept = np.append(RidgeRegr.coef_, RidgeRegr.intercept_)\n",
    "    coefsRR_sel.loc[index, :] = coeff_intercept\n",
    "    l2_lambdas_sel.append(np.linalg.norm(RidgeRegr.coef_))\n",
    "\n",
    "# Print the $ l_{2} norm $ of the coefficients\n",
    "for index, l in enumerate(lambdas_sel):\n",
    "    print(f\"For lambda = {l:10.2}, the L2-norm = {l2_lambdas_sel[index]:7.2f}\")\n",
    "    \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3760590",
   "metadata": {},
   "source": [
    "## 4. The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d51e2d",
   "metadata": {},
   "source": [
    "We have seen that ridge regression with a wise choice of $ \\lambda $ can outperform least squares as well as the null model (having only the Intercept) on the `Hitters` data set. \n",
    "We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ff7d7-85f8-4b22-81c2-170b53139919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of values for lambda, the tuning parameter\n",
    "lambdas = 10**np.linspace(-2, 3, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoEST_lambdas = Lasso(fit_intercept = True, max_iter=10000)\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefsLassoEST_lambdas = pd.DataFrame(columns = colnames)\n",
    "coefsLassoEST_lambdas[\"Intercept\"] = \"\"\n",
    "\n",
    "for index,l in enumerate(lambdas):\n",
    "    lassoEST_lambdas.set_params(alpha = l)\n",
    "    lassoEST_lambdas.fit(X_train, y_train)\n",
    "    coeff_intercept = np.append(lassoEST_lambdas.coef_, lassoEST_lambdas.intercept_)\n",
    "    coefsLassoEST_lambdas.loc[index, :] = coeff_intercept\n",
    "\n",
    "coefsLassoEST_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49848abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "plt.figure(figsize = (6, 6))\n",
    "ax = plt.axes()\n",
    "for i in range(len(coefsLassoEST_lambdas.columns)):\n",
    "    ax.plot(lambdas, coefsLassoEST_lambdas.iloc[:, i], label=coefsRR.columns[i])\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('estimated coefficients')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.75, 1), ncol=1, fontsize='small')  # Adjust bbox_to_anchor for positioning\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dfd45-818d-4045-807c-22e48b083bba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{4}$ Notice the difference with the Ridge Regression plot. Can you explain the \"kinks\"?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7102663-416d-4efb-834f-123b4dea2898",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a4300",
   "metadata": {},
   "source": [
    "## 5. Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332f2a3-40a6-4126-84df-0500fb637a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of values for lambda, the tuning parameter\n",
    "lambdas = 10**np.linspace(-2, 4, 25)\n",
    "\n",
    "elasticEST_lambdas = ElasticNet(fit_intercept = True, max_iter=10000, l1_ratio=0.5)\n",
    "\n",
    "# Create a pandas dataframe to store the coefficients\n",
    "coefsElasticNetEST_lambdas = pd.DataFrame(columns = colnames)\n",
    "coefsElasticNetEST_lambdas[\"Intercept\"] = \"\"\n",
    "\n",
    "\n",
    "for index,l in enumerate(lambdas):\n",
    "    elasticEST_lambdas.set_params(alpha = l)\n",
    "    elasticEST_lambdas.fit(X_train, y_train)\n",
    "    coeff_intercept = np.append(elasticEST_lambdas.coef_, elasticEST_lambdas.intercept_)\n",
    "    coefsElasticNetEST_lambdas.loc[index, :] = coeff_intercept\n",
    "\n",
    "coefsElasticNetEST_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bfd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients\n",
    "plt.figure(figsize = (6, 6))\n",
    "ax = plt.axes()\n",
    "for i in range(len(coefsElasticNetEST_lambdas.columns)):\n",
    "    ax.plot(lambdas, coefsElasticNetEST_lambdas.iloc[:, i], label=coefsElasticNetEST_lambdas.columns[i])\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('estimated coefficients')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.75, 1), ncol=1, fontsize='small')  # Adjust bbox_to_anchor for positioning\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06427a12-0a91-48fb-9421-127b6cdecd8a",
   "metadata": {},
   "source": [
    "## 6. A model selection task\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "\n",
    "Please carry out the following:\n",
    "1. Also carry out the ElasticNet fit with `l1_ratio` 0.25 and 0.75.\n",
    "2. For each of the five models use the validation set MSEs to determine the regularization parameter that yields the best model.\n",
    "3. Construct a table with as rows the five best models: Ridge in the first row, Lasso in the last, and the ElasticNet ones nicely ordered in between. For each, display the RMSEs for training and validation.\n",
    "\n",
    "**Notes**\n",
    "1. It is a good idea to add some code in the \"regularization loops\" to compute/store MSEs for the training set and for the validation set. See the suggestions in Exercise 6.\n",
    "2. You might be tempted to use a finer grid for the $\\lambda$'s; that's not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a077731-aa62-4c3a-818a-bef067b1197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dac10e-2be6-49fb-81c4-844a7c270047",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{5}$ Which model would you choose? Explain your choice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58f3b2c4-239c-459a-a9d7-a0c41315a804",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "    \n",
    "[//]: # (START ANSWER)\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2da2f-6b26-494b-8bfa-547cef5ebf54",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{7}$ Determine the RMSE for your best model, using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854859c-70fd-4e7c-8f1c-7d42e0298662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
