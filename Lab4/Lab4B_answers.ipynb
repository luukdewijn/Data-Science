{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4B: Logistic Regression**\n",
    "\n",
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of logistic regression.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something.\n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}.}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('lendingclub_traindata.xlsx')\n",
    "validation = pd.read_excel('lendingclub_valdata.xlsx')\n",
    "test = pd.read_excel('lendingclub_testdata.xlsx')\n",
    "\n",
    "# 1 = good, 0 = default\n",
    "\n",
    "#give column names\n",
    "cols = ['home_ownership', 'income', 'dti', 'fico', 'loan_status']\n",
    "\n",
    "train.columns = validation.columns = test.columns = cols\n",
    "\n",
    "print(train.head())\n",
    "print(\"--------------------------------\")\n",
    "print (validation.head())\n",
    "print(\"--------------------------------\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been split into training set, validation set, and test set. There are 7000 instances of the training set, 3000 instances of the validation set and 2290 instances of the test set. The four features have been labeled as: home ownership, income, dti and fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target column to create feature only dataset\n",
    "X_train = train.drop('loan_status', axis=1)\n",
    "X_val = validation.drop('loan_status', axis=1)\n",
    "X_test = test.drop('loan_status', axis=1)\n",
    "\n",
    "# Scale data using the mean and standard deviation of the training set. \n",
    "# This is not necessary for the simple logistic regression we will do here \n",
    "# but should be done if L1 or L2 regrularization is carried out\n",
    "Xmean = X_train.mean()\n",
    "Xstd = X_train.std()\n",
    "X_train = (X_train - Xmean) / Xstd\n",
    "X_val = (X_val - Xmean) / Xstd\n",
    "X_test = (X_test - Xmean) / Xstd\n",
    "\n",
    "# store target column as y-variables \n",
    "y_train = train['loan_status']\n",
    "y_val = validation['loan_status']\n",
    "y_test = test['loan_status']\n",
    "\n",
    "#print first five instances for each data set\n",
    "\n",
    "print(X_train.head())\n",
    "print(\"--------------------------------\")\n",
    "print(X_val.head())\n",
    "print(\"--------------------------------\")\n",
    "print(X_test.head())\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape,y_val.shape, X_test.shape, y_test.shape)\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = y_train.value_counts()           # count frequency of different classes in training set\n",
    "freq/sum(freq)*100                      # get percentage of above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Logistic Regression\n",
    "We use Sklearn's `LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of logisticregression named LogReg \n",
    "\n",
    "LogReg =  LogisticRegression(penalty=\"none\", solver=\"newton-cg\")     \n",
    "\n",
    "# Fit logististic regression to training set\n",
    "LogReg.fit(X_train, y_train)                                     # fit training data on logistic regression \n",
    "print(\"Parameter estimates from training data:\")\n",
    "print(LogReg.intercept_, *LogReg.coef_)                           # get the coefficients of each features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used on scaled data the model has a bias of 1.416 and coefficients of 0.145, 0.034, -0.324 and 0.363. We now test the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function used is the negative of the average log-likelihood. Below, it is computed for the validation and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_avg_loglik(y, probs):\n",
    "    # Preconditions:\n",
    "    # y: ndarray of 0/1\n",
    "    # probs: two-column ndarray, rows contain [P(Y=0),P(Y=1)]\n",
    "    return np.average(-np.log(y * probs[:,1] + (1-y) * probs[:,0]))\n",
    "    \n",
    "\n",
    "# prob_train, prob_val, and prob_test are the predicted probabilities for the training,\n",
    "# validation, and test set, using the fitted logistic regression model\n",
    "\n",
    "prob_train = LogReg.predict_proba(X_train)\n",
    "prob_val = LogReg.predict_proba(X_val)\n",
    "prob_test = LogReg.predict_proba(X_test)\n",
    "\n",
    "# Calculate the negative of the average loglikelihood for training, validation, and test set\n",
    "\n",
    "cost_func_train_minimum = neg_avg_loglik(y_train, prob_train)\n",
    "cost_func_val = neg_avg_loglik(y_val, prob_val)\n",
    "cost_func_test = neg_avg_loglik(y_test, prob_test)\n",
    "\n",
    "print('Cost function value overview:')\n",
    "print(f'minimum reached for training set = {cost_func_train_minimum:8.6}')\n",
    "print(\"From this trained model:\")\n",
    "print(f'        value for validation set = {cost_func_val:8.6}')\n",
    "print(f'              value for test set = {cost_func_test:8.6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences don't tell us very much. It would be more meaningful to compare the last two numbers to the result we would get if we computed the cost function when the validation resp the test data set are used to fit the model; those are going to be lower than the numbers above (can you see why?).\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "    \n",
    "$\\ex{1}$ a) Fit the logistic regression model to the validation data and then compute `neg_avg_loglik(y_val, prob_val_new)`, where `prob_val_new` are the predicted probabilities from the (new) fitted model. Now compute how much this differs from the results above, express this as: 2 times the difference in log-likelihood. b) Then repeat this for the test data set.\n",
    "\n",
    "The outcomes can be used to perform a formal test about the quality of the (training data) fit. The details are beyond our scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "\n",
    "# computation for validation data\n",
    "LogReg.fit(X_val,y_val)\n",
    "print(\"Parameter estimates from validation data:\")\n",
    "print(LogReg.intercept_, *LogReg.coef_)                           # get the coefficients of each features\n",
    "\n",
    "# probabilities from model fitted to validation data\n",
    "prob_val_new = LogReg.predict_proba(X_val)\n",
    "cost_func_val_minimum = neg_avg_loglik(y_val, prob_val_new)\n",
    "\n",
    "# calculate the deviance: twice the difference in log-likelihood\n",
    "deviance_val = 2*(cost_func_val - cost_func_val_minimum)*X_val.shape[0]\n",
    "\n",
    "# Calculate the negative of the average loglikelihood for training, validation, and test set\n",
    "print('\\nCost function value overview, for validation data set:')\n",
    "print(f'\\tminimum reached for validation set = {cost_func_val_minimum:8.6}')\n",
    "print(f'\\tvalue based on training model      = {cost_func_val:8.6}')\n",
    "print(f'twice the difference in loglikelihood = {deviance_val:6.2f}')\n",
    "\n",
    "\n",
    "# computation for test data\n",
    "LogReg.fit(X_test,y_test)\n",
    "print(\"\\n\\nParameter estimates from test data:\")\n",
    "print(LogReg.intercept_, *LogReg.coef_)                           # get the coefficients of each features\n",
    "\n",
    "# probabilities from model fitted to testidation data\n",
    "prob_test_new = LogReg.predict_proba(X_test)\n",
    "cost_func_test_minimum = neg_avg_loglik(y_test, prob_test_new)\n",
    "\n",
    "# calculate the deviance: twice the difference in log-likelihood\n",
    "deviance_test = 2*(cost_func_test - cost_func_test_minimum)*X_test.shape[0]\n",
    "\n",
    "# Calculate the negative of the average loglikelihood for training, testidation, and test set\n",
    "print('\\nCost function value overview, for test data set:')\n",
    "print(f'\\tminimum reached for test set       = {cost_func_test_minimum:8.6}')\n",
    "print(f'\\tvalue based on training model      = {cost_func_test:8.6}')\n",
    "print(f'twice the difference in loglikelihood = {deviance_test:6.2f}')\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "\n",
    "Some comments (visible in answers-version only):\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "**This is optional: YOU DO NOT NEED TO KNOW/UNDERSTAND THIS.**\n",
    "\n",
    "For each data set, the cost function value is lowest, when computed with the parameter from the fit to the data set itself. If we compare this to the cost function value computed from another model, the difference tells us about how well that model fits. Looking above, naturally the values based on the training model are higher (i.e., worse) that those of the best fitting model. The deviance values are to be compared to a $\\chi^2(5)$-distribution, resulting in $p$-values of about 25 and 5 percent, indicating that the model fits and generalizes well enough.\n",
    "\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An analyst must decide on a criterion for predicting whether loan will be good or default. This involves specifying a threshold By default this threshold is set to 0.5, i.e., loans are separated into good and bad categories according to whether the probability of no default is greater or less than 0.5. However this does not work well for an imbalanced data set such as this. It would predict that all loans are good! We will look at the results for few other thresholds. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{2}$ Complete the code below to get the predicted labels for each probability threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = [0.7, .75, .80, .85]\n",
    "# Create dataframe to store results\n",
    "results = pd.DataFrame(columns=[\"THRESHOLD\", \"accuracy\", \"true pos rate\", \"true neg rate\", \"false pos rate\", \"precision\", \"f-score\"]) # df to store results\n",
    "\n",
    "# Create threshold row\n",
    "results['THRESHOLD'] = THRESHOLD                                                                         \n",
    "             \n",
    "j = 0                                                                                                      \n",
    "\n",
    "# Iterate over the 3 thresholds\n",
    "\n",
    "for i in THRESHOLD:                                                                                       \n",
    "    \n",
    "    # If prob for test set > threshold predict 1\n",
    "    # START ANSWER\n",
    "    preds = np.where(LogReg.predict_proba(X_test)[:,1] > i, 1, 0)     \n",
    "    # END ANSWER                                 \n",
    "    \n",
    "    \n",
    "    # create confusion matrix \n",
    "    cm = (confusion_matrix(y_test, preds,labels=[1, 0], sample_weight=None) / len(y_test))*100    # confusion matrix (in percentage)\n",
    "    \n",
    "    print('Confusion matrix for threshold =',i)\n",
    "    print(cm)\n",
    "    print(' ')      \n",
    "    \n",
    "    TP = cm[0][0]                                                                                          # True Positives\n",
    "    FN = cm[0][1]                                                                                          # False Positives\n",
    "    FP = cm[1][0]                                                                                          # True Negatives\n",
    "    TN = cm[1][1]                                                                                          # False Negatives\n",
    "        \n",
    "    results.iloc[j,1] = accuracy_score(y_test, preds) \n",
    "    results.iloc[j,2] = recall_score(y_test, preds)\n",
    "    results.iloc[j,3] = TN/(FP+TN)                                                                         # True negative rate\n",
    "    results.iloc[j,4] = FP/(FP+TN)                                                                         # False positive rate\n",
    "    results.iloc[j,5] = precision_score(y_test, preds)\n",
    "    results.iloc[j,6] = f1_score(y_test, preds)\n",
    "   \n",
    "   \n",
    "    j += 1\n",
    "\n",
    "print('ALL METRICS')\n",
    "results.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table illustrates the trade-off that there is between the true positive rate and the false positive rate: we can improve the percentage of good loans we identify only by increasing the percentage of bad loans that are misclassified. The receiver operating curve (ROC) captures this trade-off by considering different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{3}$ Plot the ROC curve for the model and also plot the baseline that can be obtained by random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the receiver operating curve and the AUC measure\n",
    "LogReg.fit(X_train, y_train)  # make sure we have the correct trained model\n",
    "\n",
    "lr_prob=LogReg.predict_proba(X_test)\n",
    "lr_prob=lr_prob[:, 1]\n",
    "baseline_prob=[0 for _ in range(len(y_test))]\n",
    "baseline_auc=roc_auc_score(y_test, baseline_prob)\n",
    "lr_auc=roc_auc_score(y_test,lr_prob)\n",
    "print(\"AUC random predictions =\", baseline_auc)\n",
    "print(\"AUC predictions from logistic regression model =\", lr_auc)\n",
    "baseline_fpr,baseline_tpr,_=roc_curve(y_test,baseline_prob)\n",
    "lr_fpr,lr_tpr,_=roc_curve(y_test,lr_prob)\n",
    "\n",
    "# START ANSWER\n",
    "plt.plot(baseline_fpr,baseline_tpr,linestyle='--',label='Random Predction')\n",
    "plt.plot(lr_fpr,lr_tpr,marker='.',label='Logistic Regression')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Can we tell how well the model generalizes?\n",
    "There is a difficulty: how could we detect overfitting? The \"difference of log-likelihoods test\" mentioned above was an idea outside our scope. We could try this:\n",
    "\n",
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\ex{4}$ Produce and plot another ROC curve that would give you an idea about how well the fitted model generalizes. Add it to the plot above or make a new one combining the two below. Use a different color for the 2nd ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "LogReg.fit(X_train, y_train)\n",
    "lr_prob=LogReg.predict_proba(X_test)\n",
    "lr_prob=lr_prob[:, 1]\n",
    "baseline_prob=[0 for _ in range(len(y_test))]\n",
    "baseline_auc=roc_auc_score(y_test, baseline_prob)\n",
    "lr_auc=roc_auc_score(y_test,lr_prob)\n",
    "print(\"AUC random predictions =\", baseline_auc)\n",
    "print(\"AUC predictions from logistic regression model fitted with train set      =\", lr_auc)\n",
    "baseline_fpr,baseline_tpr,_=roc_curve(y_test,baseline_prob)\n",
    "lr_fpr,lr_tpr,_=roc_curve(y_test,lr_prob)\n",
    "\n",
    "\n",
    "plt.plot(baseline_fpr,baseline_tpr,linestyle='--',label='Random Predction')\n",
    "plt.plot(lr_fpr,lr_tpr,marker='.',label='LR trained on train set', color=\"r\", alpha=0.5)\n",
    "\n",
    "LogReg.fit(X_val, y_val)\n",
    "lr_prob=LogReg.predict_proba(X_test)\n",
    "lr_prob=lr_prob[:, 1]\n",
    "baseline_prob=[0 for _ in range(len(y_test))]\n",
    "lr_auc=roc_auc_score(y_test,lr_prob)\n",
    "print(\"AUC predictions from logistic regression model fitted with validation set =\", lr_auc)\n",
    "baseline_fpr,baseline_tpr,_=roc_curve(y_test,baseline_prob)\n",
    "lr_fpr,lr_tpr,_=roc_curve(y_test,lr_prob)\n",
    "\n",
    "plt.plot(lr_fpr,lr_tpr,marker='.',label='LR trained on val set', linestyle='--', color =\"g\", alpha=0.3)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#c2eafa\">\n",
    "$\\q{1}$ Explain your choice of ROC curve. State any conclusion you think you can draw from the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffa500\">\n",
    "    \n",
    "Write your answer in this colored box:\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "One possibility is to fit a logistic regression model to the validation data and then use the test data for the ROC. If the newly fitted model differs much from the original one, this might show up in the ROC curve. The only thing we might have to worry about is that the training data set is more than twice as big as the validation data set, so possibly the model fit to the validation data is not as good.\n",
    "\n",
    "We only see small differences in the ROCs and the AUC differs $0.00015$, so a miniscule amount.\n",
    "\n",
    "[//]: # (END ANSWER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
