{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daee07a5",
   "metadata": {},
   "source": [
    "# **Lab3: Linear regression with Sklearn and Statmodels**\n",
    "\n",
    "## Reproducing the results from section 3.1 and 3.2.1 of _Intro to Stat Learning_\n",
    "\n",
    "**WHAT** In this nonmandatory lab, we will look at linear regression and multiple regression for the Advertisement dataset.  We will follow the methods described in section 3.1 and 3.2.1 from _An introduction to Statistical Learning_.  We will find that `sklearn` is not equipped to produce standard errors for estimators; we'll do some \"by hand\" and for the multiple regression use `statsmodels`.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the linear regression tools, using two different libraries.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student.  Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something.\n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1.}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be23ab",
   "metadata": {},
   "source": [
    "## 1. Advertising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc460f4-1e04-49fd-93b4-04f6fd34826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advertising = pd.read_csv(\"Advertising.csv\")\n",
    "print(\"Advertising.shape =\",Advertising.shape)\n",
    "Advertising.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507d40c",
   "metadata": {},
   "source": [
    "## 2. Simple linear regression, for each of the three predictors in turn\n",
    "\n",
    "$\\ex{1}$ The code below plots for each feature a scatterplot of its value against the output (Sales). Complete the code by creating a LinearRegression model from `sklearn.linear_model.LinearRegression` and adding the regression line to each of the plots. The resulting plots are the ones from Figure 2.1 in _ISL_.\n",
    "\n",
    "Also store the residuals $e_i = y_i - \\hat{y}_i, i=1, \\ldots, n$ for TV versus Sales as TV_residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = np.array(Advertising)\n",
    "column_names = Advertising.columns\n",
    "TV_residuals = [] \n",
    "\n",
    "for i in range(3):\n",
    "    fig, ax = plt.subplots()\n",
    "    X_train = data[: , i].reshape(-1,1) # Training set samples for particular feature \n",
    "    y_train = data[: , 3] # Training set sales values \n",
    "    plt.scatter(X_train , y_train, color='red', marker = '.') #Scatter plot \n",
    "    plt.xlabel(column_names[i])\n",
    "    plt.ylabel(\"sales\")\n",
    "    \n",
    "    # START ANSWER \n",
    "    lr = LinearRegression() \n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    intcpt, slope = lr.intercept_, lr.coef_\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [slope * xlim[0] + intcpt, slope * xlim[1] + intcpt]\n",
    "    ax.plot(xlim, ylim, linewidth = 3)\n",
    "    plt.title('Regression fit of sales onto ' + column_names[i])\n",
    "    \n",
    "    if i == 0:\n",
    "        predictions = lr.predict(X_train)\n",
    "        TV_residuals = y_train - predictions\n",
    "    # END ANSWER\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d96fc",
   "metadata": {},
   "source": [
    "Recall the residual sum of squares (RSS) for simple linear regression: \n",
    "$$\\text{RSS} = e_1^2 + e_2^2 + \\ldots + e_n^2 = \n",
    "(y_1 - \\beta_0 -\\beta_1 x_1)^2 + \\cdots + (y_n - \\beta_0 -\\beta_1 x_n)^2.$$\n",
    "\n",
    "This is minimized over $(\\beta_0, \\beta_1)$ to obtain the least squares estimates.  We now make a contourplot of the RSS near the minimum, for the first model (TV), as in _ISL_ Figure 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fab75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_0s = np.linspace(5, 9, 100)\n",
    "b_1s = np.linspace(0.03, 0.06, 100)\n",
    "X_train = data[: , 0]\n",
    "\n",
    "# Initialize an array to store RSS values\n",
    "rss_values = np.zeros((len(b_0s), len(b_1s)))\n",
    "\n",
    "# Calculate RSS for each combination of B_0 and B_1\n",
    "for i, b_0 in enumerate(b_0s):\n",
    "    for j, b_1 in enumerate(b_1s):\n",
    "        y_pred = b_0 + b_1 * X_train\n",
    "        rss_values[i, j] = np.sum(np.square(y_train - y_pred)) /1000\n",
    "\n",
    "# Create a contour plot\n",
    "contour = plt.contour(b_0s, b_1s, rss_values, 30)  # levels control the number of contour lines\n",
    "plt.colorbar(contour)  # Adding a colorbar\n",
    "plt.xlabel('$\\\\beta_0$')\n",
    "plt.ylabel('$\\\\beta_1$')\n",
    "plt.title('Contour plot of RSS as function of $\\\\beta_0$ and $\\\\beta_1$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b6f8d-6325-4a99-8255-793afec7953a",
   "metadata": {},
   "source": [
    "$\\ex{2}$ Let us have a closer look at the TV fit; plot the residuals that you saved earlier in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a64410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER \n",
    "TV_points = data[: , 0] \n",
    "plt.scatter(TV_points , TV_residuals)\n",
    "plt.xlabel(\"TV\")\n",
    "plt.ylabel(\"residuals\")\n",
    "plt.show()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2fd16-ed29-4213-bdf2-f3925c0f5c09",
   "metadata": {},
   "source": [
    "$\\q{3}$ In *Introduction to Statistical Learning*, a bit after $Y = \\beta_0 + \\beta_1 X + \\epsilon$ (3.5), it says: \n",
    "> *We typically assume that the error term is independent of X.*\n",
    "\n",
    "Looking at the plot, do you think this justified for the TV-only model? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5526690-e6b4-4515-aca9-3ca6053582e2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ef8b22\">\n",
    "    \n",
    "Write your answer here:\n",
    "\n",
    "[//]: # (START ANSWER)\n",
    "The residuals seem to get larger for larger values of the TV feature, so it seems that ${\\rm Var}(\\epsilon)$ *does depend* on the predictor variable. This is called *heteroscedasticity*. What we want is *homoscedasticity*. So: no, it does not seem justified.\n",
    "\n",
    "[//]: # (END ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a488d",
   "metadata": {},
   "source": [
    "## 3. Assessing the accuracy of the coefficient estimates \n",
    "Before we actually move on to computing standard errors for our estimates, we are going to do a small simulation to get an idea where the uncertainty in the estimates comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71abe8-7953-4c59-b8ff-02b22ff1cce5",
   "metadata": {},
   "source": [
    "$\\ex{3}$ The code below plots the population regression line and the least squares estimates. Furthermore, ten least\n",
    "squares lines are shown, each computed on the basis of a separate random set of observations, as in _ISL_ Figure 3.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(8701659)\n",
    "n = 100\n",
    "xsig = 0.9\n",
    "x = rng.normal(0,xsig, n)  # normally distributed x-values\n",
    "sigma = 4\n",
    "y_true = 3 * x + 2\n",
    "\n",
    "# formula's for the standard errors (see ISL 3.1.2)\n",
    "xmean = np.mean(x)\n",
    "xvar = np.var(x)\n",
    "se_b0 = sigma * np.sqrt( (1/n) + xmean**2 / (n * xvar) )\n",
    "se_b1 = sigma / np.sqrt(  n * xvar )\n",
    "print(f\"The SEs for beta_0 and beta_1: {se_b0:.3f} and {se_b1:.3f}\") \n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "\n",
    "epsilon = rng.normal(0, sigma, n)\n",
    "y1 = y_true + epsilon\n",
    "lr = LinearRegression() \n",
    "lr.fit(x.reshape(-1,1),y1)\n",
    "\n",
    "ax[0].scatter(x, y1,  color='black', edgecolor='black', facecolor='none')\n",
    "ax[0].plot(x, y_true, color = 'red', label = \"population regression line\")\n",
    "ax[0].plot(x, lr.predict(x.reshape(-1,1)), label = f\"least squares line\", color = \"blue\") \n",
    "\n",
    "# extra runs\n",
    "for i in range(10):\n",
    "    epsilon = rng.normal(0, sigma, n)\n",
    "    y = y_true + epsilon\n",
    "    lr = LinearRegression() \n",
    "    lr.fit(x.reshape(-1,1), y)\n",
    "    ax[1].plot(x, lr.predict(x.reshape(-1,1)), color = 'lightblue', linewidth = 0.5) \n",
    "    print(f\"Regression line {i}: beta_0= {lr.intercept_:.3f}, beta_1= {lr.coef_[0]:.3f}\")\n",
    "\n",
    "lr.fit(x.reshape(-1,1),y1)\n",
    "ax[1].plot(x, y_true, color = 'red', label = \"population regression line\")\n",
    "ax[1].plot(x, lr.predict(x.reshape(-1,1)), label = f\"least squares line\", color = \"blue\")\n",
    "\n",
    "ax[0].set_ylim(-11, 13)\n",
    "ax[1].set_ylim(-11, 13)\n",
    "ax[0].set_xlim(-2.1, 2.3)\n",
    "ax[1].set_xlim(-2.1, 2.3)\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14940ab5-5256-46ed-a502-498942e13a5c",
   "metadata": {},
   "source": [
    "You can experiment by playing with the parameters `sigma`, `xsig` (controls the spread of the x's), `n` and observe the effects: what happens if you double $\\sigma$, or halve `xsig`? Do you understand what you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edfd4f",
   "metadata": {},
   "source": [
    "## 4. Advertising data: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c214cc2-7212-4677-b934-3c57e3d87346",
   "metadata": {},
   "source": [
    "## With Sklearn\n",
    "Using Sklearn's `LinearRegression` we have to compute our own standard errors, but as you can see here, this is rather cumbersome and prone to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single features \n",
    "for i in range(3):\n",
    "    lr = LinearRegression() \n",
    "    lr.fit(data[:, i].reshape(-1,1) , data[:, 3].reshape(-1,1)) \n",
    "    coefficient = lr.coef_ \n",
    "    intercept = lr.intercept_\n",
    "    mean = np.mean(data[:, i])     \n",
    "    y_pred = lr.predict(data[:, i].reshape(-1,1))\n",
    "    residuals = np.sum(np.square(data[:, 3].reshape(-1,1) - y_pred)) \n",
    "    sigma_2 = residuals / 198\n",
    "    TSS = np.sum(np.square(data[:, 3] - np.mean(data[:, 3])))\n",
    "    standard_error_b0 = np.sqrt((sigma_2 * ((1 / len(data)) + np.square(mean) / (np.sum(np.square(data[:, i] - mean))))))\n",
    "    standard_error_b1 = np.sqrt(sigma_2 / np.sum(np.square(data[:, i] - mean)))\n",
    "    t_statistic_coefficient =  coefficient[0][0] / standard_error_b0\n",
    "    print(f\"coefficient of {column_names[i]} is {coefficient[0][0].round(4)} with SE {standard_error_b1.round(4)} and intercept is {intercept[0].round(4)} with SE {standard_error_b0.round(4)}\")\n",
    "    print(f\"It has an RSE of {np.sqrt(residuals / 198).round(3)} and an R^2 of {(1 - residuals/TSS).round(3)} \")\n",
    "    \n",
    "print()\n",
    "\n",
    "# Multiple features     \n",
    "lr = LinearRegression() \n",
    "lr.fit(data[:, :3] , data[:, 3].reshape(-1,1)) \n",
    "coefficients = lr.coef_\n",
    "intercept = lr.intercept_\n",
    "y_pred = lr.predict(data[:, :3])\n",
    "residuals = np.sum(np.square(data[:, 3].reshape(-1,1) - y_pred)) \n",
    "sigma_2 = residuals / 198\n",
    "for i in range(3):\n",
    "    mean = np.mean(data[:, i])\n",
    "    standard_error_b1 = np.sqrt(sigma_2 / np.sum(np.square(data[:, i] - mean)))\n",
    "    t_statistic = coefficients[0][i] / standard_error_b1\n",
    "    print(f\"coefficient of  {column_names[i]} is {coefficients[0][i].round(4)} with SE of {standard_error_b1.round(4)} and t-statistic {t_statistic.round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b0daf-99d0-4dbd-81b1-6eac0ead4b53",
   "metadata": {},
   "source": [
    "## With Statsmodels\n",
    "Statsmodels has methods that have builtin the determination of the statistics we are after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a35e79-fcd4-4e20-a3c4-697ca0c6111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imports\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035a8c0",
   "metadata": {},
   "source": [
    "$\\ex{4}$ Using `Statsmodels`'s `OLS`, fit the models using \n",
    ">1. the features independently \n",
    "2. all the features together\n",
    "\n",
    "to compute feature coefficients, standard errors, t-statistics, residual standard errors and $R^2$. You will need to build an OLS model and then `fit()` will produce a `RegressionResults` object with all you need. In the documentation, look for methods with \"summary\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c6610-25e1-4149-ad3c-476b4f1fdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TV\n",
    "# START ANSWER \n",
    "y = Advertising['sales']\n",
    "X = pd.DataFrame({'intercept': np.ones(Advertising.shape[0]), 'TV': Advertising['TV']})\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# fig, ax = subplots()\n",
    "ax =  Advertising.plot.scatter('TV', 'sales', color = 'red', marker = '.')\n",
    "intcpt, slope = results.params\n",
    "xlim = ax.get_xlim()\n",
    "ylim = [slope * xlim[0] + intcpt, slope * xlim[1] + intcpt]\n",
    "ax.plot(xlim, ylim, linewidth = 3)\n",
    "\n",
    "results.summary2()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f90a5a-fe50-41f8-b39c-e324bb918bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radio\n",
    "# START ANSWER\n",
    "y = Advertising['sales']\n",
    "X = sm.add_constant(Advertising['radio'])\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8c316-2945-4f98-9eaa-327a0726039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newspaper\n",
    "# START ANSWER\n",
    "y = Advertising['sales']\n",
    "X = sm.add_constant(Advertising['newspaper'])\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary2()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fbac7-7ba0-4afe-8d47-96c1b6537de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all three predictors\n",
    "# START ANSWER \n",
    "y = Advertising['sales']\n",
    "X = sm.add_constant(Advertising.iloc[:,:3])\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary2()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99476c-79d4-46cd-9b77-882384812d0b",
   "metadata": {},
   "source": [
    "$\\ex{5} Compute the covariance matrix and check it against the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
